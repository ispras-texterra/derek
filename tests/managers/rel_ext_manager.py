import unittest

from derek.data.model import Document, Sentence, Paragraph, Entity, Relation
from derek.rel_ext import RelExtTrainer
from tests.test_helper import get_training_hook


class TestRelExtManager(unittest.TestCase):
    def setUp(self) -> None:
        self.docs = []

        # BB-event-4329237
        tokens = [
            "The", "in", "vitro", "assay", "of", "tuberculin", "hypersensitivity", "in", "Macaca", "mulatta",
            "sensitized", "with", "bacille", "Calmette", "Guerin", "cell", "wall", "vaccine", "and-or", "infected",
            "with", "virulent", "Mycobacterium", "tuberculosis", "."
        ]
        sentences = [Sentence(0, 25)]
        paragraphs = [Paragraph(0, 1)]
        entities = [
            Entity("T2", 8, 18, "Habitat"),
            Entity("T3", 8, 24, "Habitat"),
            Entity("T4", 12, 18, "Habitat"),
            Entity("T5", 12, 15, "Bacteria"),
            Entity("T6", 22, 24, "Bacteria")
        ]
        relations = {Relation(entities[4], entities[1], "Lives_In")}

        # token features generated by UDPipe
        pos = [
            'DET', 'ADP', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'ADP', 'PROPN', 'PROPN',
            'VERB', 'ADP', 'PROPN', 'PROPN', 'PROPN', 'NOUN', 'NOUN', 'NUM', 'NOUN', 'VERB',
            'ADP', 'ADJ', 'PROPN', 'NOUN', 'PUNCT'
        ]

        dt_labels = [
            'det', 'case', 'compound', 'nsubj', 'case', 'compound', 'nmod', 'case', 'compound', 'nmod',
            'root', 'case', 'compound', 'flat', 'compound', 'compound', 'obl', 'nummod', 'appos', 'acl',
            'case', 'amod', 'compound', 'obl', 'punct'
        ]

        dt_head_distances = [
            3, 2, 1, 7, 2, 1, -3, 2, 1, -6,
            0, 5, 2, -1, 2, 1, -6, 1, -2, -1,
            3, 2, 1, -4, -14
        ]

        token_features = {"pos": pos, "dt_labels": dt_labels, "dt_head_distances": dt_head_distances}
        self.docs.append(Document("_", tokens, sentences, paragraphs, entities, relations, token_features))

        # BB-event-9564489
        tokens = [
            'Gingivomandibular', 'infection', 'due', 'to', 'Mycobacterium', 'kansasii',
            'in', 'a', 'patient', 'with', 'AIDS', '.'
        ]
        sentences = [Sentence(0, 12)]
        paragraphs = [Paragraph(0, 1)]
        entities = [
            Entity("T2", 0, 1, "Habitat"),
            Entity("T3", 4, 6, "Bacteria"),
            Entity("T4", 8, 11, "Habitat")
        ]
        relations = {
            Relation(entities[1], entities[0], "Lives_In"),
            Relation(entities[1], entities[2], "Lives_In")
        }

        # token features generated by UDPipe
        pos = [
            'ADJ', 'NOUN', 'ADP', 'ADP', 'PROPN', 'PROPN',
            'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT'
        ]

        dt_labels = [
            'amod', 'root', 'case', 'fixed', 'compound', 'nmod',
            'case', 'det', 'nmod', 'case', 'nmod', 'punct'
        ]

        dt_head_distances = [
            1, 0, 3, -1, 1, -4,
            2, 1, -7, 1, -2, -10
        ]

        token_features = {"pos": pos, "dt_labels": dt_labels, "dt_head_distances": dt_head_distances}
        self.docs.append(Document("_", tokens, sentences, paragraphs, entities, relations, token_features))
        self.docs_no_rels = [doc.without_relations() for doc in self.docs]
        self.props = {
            "shared": {
                "internal_emb_size": 10,
                "token_position_size": 10,
                "max_word_distance": 20,

                "dt_distance_emb_size": 10,
                "max_dt_distance": 10,

                "dt_depth_emb_size": 10,
                "max_dt_depth": 10,

                "pos_emb_size": 10
            },

            "add_we": "true",
            "add_shared": "true",

            "optimizer": "adam",
            "learning_rate": 0.01,

            "epoch": 2,
            "loss": "cross_entropy",
            "l2": 0.0001,

            "lr_decay": 0.1,
            "dropout": 0.5,
            "clip_norm": 1,

            "max_candidate_distance": 20,
            "batch_size": 8,

            "token_position_size": 10,
            "max_word_distance": 10,

            "encoding_size": 10,
            "entities_types_emb_size": 20,

            "entities_depth_emb_size": 10,
            'max_entities_depth': 2,

            "specific_encoder_size": 10,

            "aggregation": {"attention": {}, "max_pooling": {}, "mean_pooling": {}, "take_spans": {}, "last_hiddens": {}},

            "seed": 100
        }

        # GENIA id=10022435
        tokens = [
            "Glucocorticoid", "resistance", "in", "the", "squirrel", "monkey", "is", "associated",
            "with", "overexpression", "of", "the", "immunophilin", "FKBP51", "."
        ]
        sentences = [Sentence(0, 15)]
        paragraphs = [Paragraph(0, 1)]

        pos = [
            "NN", "NN", "IN", "DT", "NN", "NN", "VBZ", "VBN",
            "IN", "NN", "IN", "DT", "NN", "NN", "PERIOD"
        ]

        dt_labels = [
            "compound", "nsubjpass", "case", "det", "compound", "nmod", "auxpass", "root",
            "case", "nmod", "case", "det", "compound", "nmod", "dep"
        ]

        dt_head_distances = [
            1, 6, 3, 2, 1, -4, 1, 0,
            1, -2, 3, 2, 1, -4, -7
        ]

        token_features = {"pos": pos, "dt_labels": dt_labels, "dt_head_distances": dt_head_distances}
        self.unlabeled_docs = [Document("_", tokens, sentences, paragraphs, token_features=token_features)]

        self.sdp_config = {
                "context_encoding_non_linearity_size": 10,
                "loss": "cross_entropy",
                "learning_rate": 0.02,
                "query_dense_size": 10,
                "clip_norm": 1,
                "batch_size": 1
        }

        self.parser_config = {
            "context_encoding_non_linearity_size": 10,
            "loss": "cross_entropy",
            "learning_rate": 0.02,
            "clip_norm": 1,
            "batch_size": 1,

            "add_shared": True,
            "specific_encoder_size": 10,

            "sampling_strategy": "pos_filtering",
            "arc_token_distance_in_classifier_size": 10,
            "arc_token_distance_in_attention_size": 10,
            "max_arc_token_distance": 10,
            "aggregation": {"attention": {"type": "luong", "normalise_coefficients": True}, "take_spans": {}}
        }

    def _train(self, props, with_unlabeled=True):
        hook, lst = get_training_hook(self.docs_no_rels)

        with RelExtTrainer(props) as trainer:
            trainer.train(self.docs, unlabeled_docs=self.unlabeled_docs if with_unlabeled else None, early_stopping_callback=hook)

        # validate that hook was called after each epoch
        self.assertEqual(lst, [True] * props["epoch"])

    def test_without_auxiliary(self):
        self._train(self.props, False)

    def test_no_unlabeled_docs_with_auxiliary(self):
        for task, config in zip(("sdp", "parser"), (self.sdp_config, self.parser_config)):
            props = {**self.props, f"{task}_config": config}
            hook, lst = get_training_hook(self.docs_no_rels)

            with RelExtTrainer(props) as trainer:
                self.assertRaises(Exception, trainer.train, self.docs, None, hook)
            self.assertEqual(lst, [])

    def test_with_sdp_pretrain(self):
        props = {**self.props, "sdp_config": self.sdp_config, "sdp_pretraining_epoch": 1}
        self._train(props)

    def test_with_sdp_multitask(self):
        props = {**self.props, "sdp_config": self.sdp_config, "sdp_samples_ratio": 1}
        self._train(props)

    def test_with_sdp_pretrain_multitask(self):
        props = {**self.props, "sdp_config": self.sdp_config, "sdp_samples_ratio": 1, "sdp_pretraining_epoch": 1}
        self._train(props)

    def test_with_parser_pretrain(self):
        props = {**self.props, "parser_config": self.parser_config, "parser_pretraining_epoch": 1}
        self._train(props)

    def test_with_parser_multitask(self):
        props = {**self.props, "parser_config": self.parser_config, "parser_samples_ratio": 1}
        self._train(props)

    def test_with_parser_pretrain_multitask(self):
        props = {
            **self.props, "parser_config": self.parser_config, "parser_samples_ratio": 1, "parser_pretraining_epoch": 1
        }
        self._train(props)

    def test_with_parser_sdp(self):
        props = {
            **self.props, "parser_config": self.parser_config, "parser_samples_ratio": 1, "parser_pretraining_epoch": 1,
            "sdp_config": self.sdp_config, "sdp_samples_ratio": 2, "sdp_pretraining_epoch": 1
        }
        self._train(props)
